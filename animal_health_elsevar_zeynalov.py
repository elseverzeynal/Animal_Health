# -*- coding: utf-8 -*-
"""Animal Health - Elsevar Zeynalov.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Aous65PzhYassgx9EfX53FPCgrOuo5RP

<h1 align="center">
    NSDC Data Science Projects
</h1>
  
<h2 align="center">
    Project: Animal Health - Data Project
</h2>

<h3 align="center">
    Name: Elsevar & Elvin Zeynalov
</h3>

**About the Project**

The Animal Health project will focus on the conservation of wildlife, with a mission to protect, restore, and enhance natural ecosystems. This initiative will aim to develop a predictive model that will identify whether an animal's condition is dangerous and if it is at risk of dying, drawing upon five distinct symptoms. The dataset for this project, sourced from Kaggle, will feature a diverse array of species ranging from birds to mammals, each characterized by symptoms such as fever, coughing, weight loss, pain, and more.

A Random Forest Classifier will be implemented to classify whether an animal is in danger. The dataset will undergo thorough data cleaning to refine it for model training. To address the challenge of an unbalanced dataset, the Random Over Sampling method will be employed to achieve balance. Exploratory data analysis will uncover significant insights into the danger levels of the animals, which will inform decision-making for animal welfare and contribute to bio-heritage conservation.

In addition to the Random Forest model, other machine learning models will be implemented to enhance predictive accuracy. By improving the understanding and management of animal health, this project will contribute to maintaining wildlife populations and promoting biodiversity.

## Milestone 1: Importing Libraries and Dataset

**Goal:** The aim is to import the necessary libraries and load the animal health dataset into the environment. This step is crucial as it sets up the foundational tools and data for the project.

**Step 1:** Import Libraries
"""

#TODO: Import Libraries
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

import string
from difflib import SequenceMatcher
from textblob import TextBlob

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

import warnings
warnings.filterwarnings("ignore")

"""**Step 2:** Import Dataset from Kaggle

Dataset URL: https://www.kaggle.com/datasets/gracehephzibahm/animal-disease
"""

##TODO:install kaggle
!pip install -q kaggle

#Import and upload your files below:
from google.colab import files
files.upload()

!kaggle datasets download -d gracehephzibahm/animal-disease

!unzip animal-disease.zip

"""## Milestone 2: Exploratory Data Analysis (EDA)

**Goal:** The focus of this milestone is to analyze the dataset to uncover patterns, relationships, and trends within the data. This analysis helps inform the model-building process.

**Step 1:** Read the data and print the data
"""

#TODO: Read the csv and print them
data = pd.read_csv("/content/data.csv")

#TODO: Understand the type of data we are using
type(data)

#TODO: Review the shape of the data
data.shape

#TODO: Review the column names within the dataset
data.columns

"""**Step 2:** Use the head function to view the top 5 rows of your data."""

#TODO: Fill in below using the head function
data.head()

"""**Step 3:** Use the tail function to view the bottom 5 rows of your data."""

#TODO: Fill in below using the tail function
data.tail()

#TODO: Run the dtypes function
data.dtypes

"""**Step 4:** Use the info function to view the information of your data."""

#TODO: Fill in below using the info function
data.info()

"""## Milestone 3: Data Cleaning and Preprocessing

**Goal:** The objective here is to clean the dataset, handle missing or erroneous data, and prepare it for analysis and modeling. Effective preprocessing improves model performance and reliability.

[Read this article](https://www.knowledgehut.com/blog/data-science/data-cleaning#conclusion-%C2%A0) to learn more about the process!

There are two rows with null value in the dangerous column
"""

data[data['Dangerous'].isna()]

"""**Step 1:** Check each column to see if there are any other NULL values"""

data[(data['symptoms1'] == 'Teeth griding')]

data[(data['symptoms2'] == 'Apathy') ]

data[(data['symptoms3'] == 'Dehydration')]

data[(data['symptoms4'] == 'Ruminal stasis')]

data[(data['symptoms5'] == 'Watery faeces')]

"""**Step 2:** Columns like teeth grinding, apathy, ruminal stasis and watery feces do not have enough data, so we'll drop those for simplicity."""

#TODO: Drop the columns with missing values for simplicity
data.dropna(inplace = True, axis = 0)

#TODO: Let's look at the data's shape again
data.shape

"""**Step 3:** Handle the Animal Names - use the unique function to count different types of animals"""

#TODO: Use the unique function on the animal names
data["AnimalName"].unique()

#TODO: Count each type of animal name
data['AnimalName'].value_counts()

"""**Step 4:** Modify the repeated names and convert them to lower case and uniform the names of animals. For example, "goats" to "goat" and "moos" to "cow."
"""

AnimalName = list(data['AnimalName'].unique())
AnimalName.sort()
AnimalName

#TODO: Replace animal names, as needed, to create a more uniform list of animals
data['AnimalName'].replace({'black-tailed deer':'deer','white-tailed deer':'deer','mule deer':'deer','sika deer':'deer',
                            'other birds': 'bird', 'dogs': 'dog', 'fox ':'fox', 'goats': 'goat', 'moos': 'cow', 'birds': 'bird',
                            'chicken': 'hen', 'fowl': 'bird', 'buffaloes': 'buffalo', 'hyaenas': 'hyaena', 'pigs': 'pig','wolves': 'wolf',
                            'mules': 'mule'}, inplace = True)

#TODO: Make the animal names lower case and sort
# Convert names to lowercase
data['AnimalName'] = data['AnimalName'].str.lower()
# Get unique and sorted animal names
animalName = sorted(data['AnimalName'].unique())
# Display sorted unique animal names
animalName

data['Dangerous'].unique()

"""**Step 5:** Handle the Repetitive Symptoms and convert it to lower case


"""

cols = list(data.columns)
for col in cols:
    data[col] = data[col].str.lower()

s1 = set(data["symptoms1"].unique())
s2 = set(data["symptoms2"].unique())
s3 = set(data["symptoms3"].unique())
s4 = set(data["symptoms4"].unique())
s5 = set(data["symptoms5"].unique())

# Checking the total number of unique symptoms
s_total = s1.union(s2).union(s3).union(s3).union(s4)
len(s_total)

"""**Step 6:** Remove Unwanted Spaces"""

characters_unique = set()
cols = list(data.columns)
for col in cols:
    for value in data[col]:
        for letter in value:
            characters_unique.add(letter)

list(sorted(characters_unique))

"""**Step 7:** Remove the special characters"""

temp = []
for ele in characters_unique:
    if ele not in string.ascii_lowercase:
        temp.append(ele)
temp

import string

# TODO: Creating a cleaning function
def clean(char):
    # Unwanted characters are being changed to space
    for extra in ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '=', '+', '[', ']', '{', '}', '|', '\\', ':', ';', '"', "'", '<', '>', ',', '.', '?', '/', '~', '`', '\n', '\t']:
        char = char.replace(extra, ' ')

    list_char = char.split()
    final_char = ""
    for ele in list_char:
        ele = ele.strip()

        flag = True
        for letter in ele:
            if letter not in string.ascii_lowercase:
                flag = False

        if flag:
            final_char += ele + " "

    return final_char.strip()

# TODO: Perform cleaning function on all columns
cols = list(data.columns)
for col in cols:
    data[col] = data[col].apply(clean)

# TODO: Recheck unique count
s1 = set(data["symptoms1"].unique())
s2 = set(data["symptoms2"].unique())
s3 = set(data["symptoms3"].unique())
s4 = set(data["symptoms4"].unique())
s5 = set(data["symptoms5"].unique())

# TODO: Checking the total number of unique symptoms
s_total = s1.union(s2, s3, s4, s5) #look at Step 5 for a hint!
len(s_total)

# TODO: Renaming the columns
data.rename(columns={'AnimalName': 'Animal',
                   'symptoms1': 'Symptom 1',
                   'symptoms2': 'Symptom 2',
                   'symptoms3': 'Symptom 3',
                   'symptoms4': 'Symptom 4',
                   'symptoms5': 'Symptom 5'}, inplace=True)

"""**Step 8:** Handle Spelling Errors and count spelling errors"""

def similar(a, b):
    return SequenceMatcher(None, a, b).ratio()

# Testing
similar("Apple","Appel")

# TODO: Testing with actual value in the dataframe
similar('abdminal pain', 'abdominal pain')

def change(data, ws, new):
    symps = [0, 1, 2, 3, 4]
    for col in symps:
        for w in ws:
            data.iloc[:, col] = data.iloc[:, col].replace(w, new)
    return data

# TODO: Structuring the words and the output
s_total = list(s_total)

similarity_scores = []
for i1, word1 in enumerate(s_total):
    for i2, word2 in enumerate(s_total[i1+1:]):
        ratio = similar(word1, word2)
        if ratio > 0.75: # Keep 0.75 as threshold
            value = [ratio, (word1, word2)]
            similarity_scores.append(value)

similarity_scores.sort(key=lambda val: val[0], reverse = True)
len(similarity_scores)
# There are many spelling mistakes

# Let us do some of the next steps for you! Fill in the blanks to create "new" words that account for all of the mispellings. Review an example or two below.
data = change(data = data, ws = ['difficultty in breathing', 'difficulty in breathing',
                           'difficulty breating', 'difficulty breathing',
                           'diffculty breathing', 'difficulty breathing',
                           'labored breathing', 'lound breathing',
                           'respiratory noise', 'respiratory distress',
                          'gasping for breath', 'gasping for air'], new = 'breathing difficulty')
data = change(data = data, ws = ['difficulty in walking', 'difficult in walking',
                           'limp', 'lip', 'inability to stand',
                           'difficulty in walk', 'difficulty walking', 'walking problem',
                           'lameness', 'legness'], new = 'walking difficulty')
data = change(data = data, ws = ['high body temperaure', 'high body temperature'], new = 'high body temperature')
data = change(data = data, ws = ['decreased appetite', 'decrease appetite', 'poor appetite',
                           'loss of eat', 'loss of appettite', 'loss of appetite', 'reduced appetite',
                           'loss od appetite', 'loss of appetite', 'unable to eat',
                           'loss of appetite', 'lack of appetite'], new = 'decreased appetite')
data = change(data = data, ws = ['ocular discharge', 'occular discharge',
                           'eye discharges', 'eye disharge',
                           'discharge from eyes', 'mucus discharge from the eye'], new = 'ocular discharge')
data = change(data = data, ws = ['inappetence', 'inappentence'], new = 'inappetence')
data = change(data = data, ws = ['skin reashes', 'skin rashes'], new = 'skin rashes')
data = change(data = data, ws = ['nosebleeds', 'nose bleeds',
                           'nose bleeds', 'nosebleed'], new = 'nose bleeds')
data = change(data = data, ws = ['despression', 'depression'], new = 'depression')
data = change(data = data, ws = ['weightloss', 'weight loss'], new = 'weight loss')
data = change(data = data, ws = ['watery eyes', 'watery eye'], new = 'watery eyes')
data = change(data = data, ws = ['diffulty swallowing', 'difficulty swallowing'], new = 'difficulty swallowing')
data = change(data = data, ws = ['vomitting', 'vomiting'], new = 'vomit')
data = change(data = data, ws = ['dizzines', 'dizziness'], new = 'dizziness')
data = change(data = data, ws = ['dullness', 'dull ness'], new = 'dullness')
data = change(data = data, ws = ['diarrhea', 'diarrhoea'], new = 'diarrhoea')
data = change(data = data, ws = ['pneumonia', 'pnemonia'], new = 'pneumonia')
data = change(data = data, ws = ['bloody diarrhea', 'bloody diarhhea'], new = 'bloody diarrhoea')
data = change(data = data, ws = ['watery eyes', 'watery eye'], new = 'watery eyes')
data = change(data = data, ws = ['diffulty swallowing', 'difficulty swallowing',
                           'difficulty in swallowing', 'difficulty swallowing'], new = 'difficulty swallowing')
data = change(data = data, ws = ['abnormalities', 'abnormalalities'], new = 'abnormalities')
data = change(data = data, ws = ['blood in faces', 'blood on faces',
                           'blood stool', 'blood in stool'], new = 'bloody faces')
data = change(data = data, ws = ['tremor', 'tremors'], new = 'tremor')
data = change(data = data, ws = ['anemia', 'aneamia',
                          'anemia', 'anaemia'], new = 'anemia')
data = change(data = data, ws = ['hyperesthesia', 'hyperaestesia'], new = 'hyperesthesia')
data = change(data = data, ws = ['attack', 'attacks'], new = 'attack')
data = change(data = data, ws = ['lesion', 'lesions',
                          'lession on the skin', 'lession on cat skin'], new = 'lesion')
data = change(data = data, ws = ['excess salivation', 'excession salivation',
                           'excessive grooming', 'excessive drooling',
                           'excess salivation', 'excess salivary'], new = 'excess salivation')
data = change(data = data, ws = ['nausea', 'nause'], new = 'nausea')
data = change(data = data, ws = ['edema', 'oedema'], new = 'edema')
data = change(data = data, ws = ['ulcers', 'ulcer'], new = 'ulcer')
data = change(data = data, ws = ['sweat', 'sweats',
                           'sweating'], new = 'sweat')
data = change(data = data, ws = ['grinding teeth', 'grinding of teeth'], new = 'grinding of teeth')
data = change(data = data, ws = ['scratching', 'scartching'], new = 'scratching')
data = change(data = data, ws = ['salivating', 'salivation'], new = 'salivation')
data = change(data = data, ws = ['week pulse', 'weak pulse'], new = 'weak pulse')
data = change(data = data, ws = ['discharge from eye', 'discharge from eyes'], new = 'eye discharge')
data = change(data = data, ws = ['skin color change', 'skin colour change'], new = 'skin color change')
data = change(data = data, ws = ['aversion to light', 'anversion to light'], new = 'aversion to light')
data = change(data = data, ws = ['fluffed feather', 'fluffed feathers'], new = 'fluffed feathers')
data = change(data = data, ws = ['abdominal pain', 'abdminal pain',
                           'abdonormal discomfort', 'abdominal discomfort',
                           'abdonormal pain', 'abdominal pain'], new = 'abdominal pain')
data = change(data = data, ws = ['swelling of joints', 'swelling on joints'], new = 'joint swelling')
data = change(data = data, ws = ['head ache', 'headache',
                           'head tossing', 'head pressing'], new = 'headache')
data = change(data = data, ws = ['muscles ache', 'muscle aches'], new = 'muscle ache')
data = change(data = data, ws = ['join pains', 'joint pain'], new = 'joint pain')

# Similarity score >= 0.8 and <0.9
data = change(data = data, ws = ['oains', 'pain',
                           'pain', 'pains'], new = 'pain')
data = change(data = data, ws = ['shaking oh head', 'shaking head',
                            'head shking', 'head shaking'], new = 'head shaking')
data = change(data = data, ws = ['scratches', 'scartches'], new = 'scratching')
data = change(data = data, ws = ['muscle stiffness', 'muscular stiffness'], new = 'muscle stiffness')
data = change(data = data, ws = ['ruffled feathers', 'fluffed feathers',
                           'puffed up feather', 'ruffled feathers'], new = 'ruffled feathers')
data = change(data = data, ws = ['seizuers', 'seizures'], new = 'seizures')
data = change(data = data, ws = ['lathargy'], new = 'lethargy')
data = change(data = data, ws = ['weekness', 'weakness'], new = 'weakness')
data = change(data = data, ws = ['bleeding wounds', 'bleeding from wounds'], new = 'bleeding from wounds')
data = change(data = data, ws = ['gasc', 'gas'], new = 'gas')
data = change(data = data, ws = ['high temperature', 'high body temperature'], new = 'high body temperature')
data = change(data = data, ws = ['fatigue', 'fatique'], new = 'fatigue')
data = change(data = data, ws = ['distress', 'stress'], new = 'stress')
data = change(data = data, ws = ['poor coat condition', 'poor condition',
                           'poor condition', 'poor body condition',
                           'poor coat condition', 'poor body condition'], new = 'poor body condition')
data = change(data = data, ws = ['relunctance to move', 'reluctant move'], new = 'reluctant move')
data = change(data = data, ws = ['bloody urine', 'blood in urine'], new = 'bloody urine')
data = change(data = data, ws = ['change in gait', 'changed gait'], new = 'change in gait')
data = change(data = data, ws = ['drop in milk production', 'decrease in milk production'], new = 'decrease in milk production')
data = change(data = data, ws = ['thivk skin', 'thicked skin'], new = 'thick skin')
data = change(data = data, ws = ['tear produce', 'tear production'], new = 'tear production')
data = change(data = data, ws = ['rapid heartbeats', 'rapid heart rate'], new = 'rapid heartbeats')
data = change(data = data, ws = ['flock moratality', 'kid moratality'], new = 'flock moratality')
data = change(data = data, ws = ['skin irritation', 'irritation'], new = 'skin irritation')
data = change(data = data, ws = ['itchiness', 'itches', 'itching'], new = 'itchiness')
data = change(data = data, ws = ['listless', 'listlessness'], new = 'listless')
data = change(data = data, ws = ['drooping wings', 'droopy wings'], new = 'droopy wings')
data = change(data = data, ws = ['tachypea', 'trachea'], new = 'trachea')
data = change(data = data, ws = ['hot joints', 'hock joint'], new = 'hot joints')
data = change(data = data, ws = ['cough', 'coughing'], new = 'cough')
data = change(data = data, ws = ['swelling on thebody'], new = 'swelling on the body')

"""**Step 9:** Now try and view the top 5 rows of your dataset"""

#Use the head function below
data.head()

"""**Step 10:** Recheck the unique count"""

# TODO: Rechecking unique count
s1 = set(data["Symptom 1"].unique())
s2 = set(data["Symptom 2"].unique())
s3 = set(data["Symptom 3"].unique())
s4 = set(data["Symptom 4"].unique())
s5 = set(data["Symptom 5"].unique())

# Checking the total number of unique symptoms
s_total = s1.union(s2).union(s3).union(s4).union(s5)
len(s_total)

"""## Milestone 4: Addressing Imbalanced Dataset

**Goal:** Implement techniques such as Random Over Sampling to address the issue of dataset imbalance. A balanced dataset leads to more robust model performance.

**Step 1:** Install Imbalanced-Learn
"""

! pip install -U imbalanced-learn

"""**Step 2**: Drop the dangerous column and visualise the data"""

#Use data.drop to drop the dangerous column
X = data.drop(['Dangerous'], axis = 1)
Y = data['Dangerous']

Y.value_counts()

#show pie chart
Y.value_counts().plot.pie(autopct= '%.2f')

"""**Step 3**: Import and apply RandomOverSampler

New to RandomOverSampler and Imbalanced-Learn? Read this Medium article - https://lerekoqholosha9.medium.com/random-oversampling-and-undersampling-for-imbalanced-classification-a4aad406fd72
"""

#TODO: Import RandomOverSampler and fit
from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(sampling_strategy = "not majority")#string
X_res , Y_res = ros.fit_resample(X,Y)

ax = Y_res.value_counts().plot.pie(autopct= '%.2f')
_ = ax.set_title("Oversampling")

"""**Step 4**: Look at the input features and print data"""

data

"""**Step 5:** Input and apply LabelEncoder. Need a refresher on LabelEncoder in Python? Check out this resource: https://www.youtube.com/watch?v=YvEx0IGKTko"""

#TODO: Import and apply Label Encoder
from sklearn.preprocessing import LabelEncoder
LE = LabelEncoder()
data['enc-Symptom 1'] = LE.fit_transform(data['Symptom 1'])
data['enc-Symptom 2'] = LE.fit_transform(data['Symptom 2'])
data['enc-Symptom 3'] = LE.fit_transform(data['Symptom 3'])
data['enc-Symptom 4'] = LE.fit_transform(data['Symptom 4'])
data['enc-Symptom 5'] = LE.fit_transform(data['Symptom 5'])
data['enc-Dangerous'] = LE.fit_transform(data['Dangerous'])
X=data[['enc-Symptom 1', 'enc-Symptom 2', 'enc-Symptom 3', 'enc-Symptom 4', 'enc-Symptom 5']]
Y=data['enc-Dangerous']
X.shape
Y.shape
X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size = 0.2)
X_train.shape, Y_train.shape
X_test.shape, Y_test.shape
clf = RandomForestClassifier()
clf.fit(X_train, Y_train)
X.iloc[0]
print(clf.predict([[68,36,193,193,31]]))
print(clf.predict_proba([[68,36,193,193,31]]))
print(clf.predict(X_test))
print(clf.predict_proba([[68,36,193,193,31]]))
print(clf.predict(X_test))
print(Y_test)
print(clf.score(X_test, Y_test))

"""LabelEncoder is a tool from the Scikit-learn library in Python that transforms categorical labels (like 'red', 'green', and 'blue') into numerical values, making it easier for machine learning algorithms to process the data. It assigns a unique integer to each label, allowing algorithms to work with numbers rather than text. For example, it might convert 'red' to 0, 'green' to 1, and 'blue' to 2. This process helps in preparing data for analysis and modeling, especially when dealing with algorithms that require numerical inputs."""

LE = LabelEncoder()

data['enc-Symptom 1'] = LE.fit_transform(data['Symptom 1'])
data['enc-Symptom 2'] = LE.fit_transform(data['Symptom 2'])
data['enc-Symptom 3'] = LE.fit_transform(data['Symptom 3'])
data['enc-Symptom 4'] = LE.fit_transform(data['Symptom 4'])
data['enc-Symptom 5'] = LE.fit_transform(data['Symptom 5'])
data['enc-Dangerous'] = LE.fit_transform(data['Dangerous'])

X=data[['enc-Symptom 1', 'enc-Symptom 2', 'enc-Symptom 3', 'enc-Symptom 4', 'enc-Symptom 5']]

Y=data['Dangerous']

#TODO: Review the shape
X.shape

#TODO: Review the shape
Y.shape

"""**Step 6:** Split the Dataset"""

#TODO: Split the dataset
X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size = 0.2)

X_train.shape, Y_train.shape

X_test.shape, Y_test.shape

"""## Milestone 5: Model Training - Random Forest Classifier

**Goal:** Train a Random Forest Classifier to predict whether an animal is in danger based on the provided symptoms. This model serves as a primary classification tool for the project.

Check out the resource on Random Forest Classifier- https://www.youtube.com/watch?v=fWKM6x7RnAQ

**Step 1**: Fit the Random Forest Classifier with training data
"""

clf = RandomForestClassifier()

#TODO: Fit randomforestclassifier
clf.fit(X_train, Y_train)

"""**Step 2:** Perform prediction single sample from dataset"""

X.iloc[0]

print(clf.predict([[68,36,193,193,31]]))

print(clf.predict_proba([[68,36,193,193,31]]))

"""**Step 3:** Perform prediction on the test dataset

These are PREDICTED CLASS LABELS
"""

print(clf.predict(X_test))

"""These are ACTUAL CLASS LABELS"""

print(Y_test)

"""**Step 4:** Check model performance and accuracy"""

print(clf.score(X_test, Y_test))

"""## Milestone 6: Comparing Other Machine Learning Models

**Goal:** Compare additional machine learning models to the Random Forest to potentially improve predictive accuracy. This milestone focuses on exploring alternatives for enhanced performance.

**Step 1:** Train and Test Logistic Regression Model

Check out the resource on Logistic Regression from IBM - https://www.ibm.com/topics/logistic-regression

**Step 2:** Check Classification Report and Confusion Matrix

Want to learn about evaluating your Machine Learning Models. Check out - https://smuhabdullah.medium.com/confusion-matrices-and-classification-reports-a-guide-to-evaluating-machine-learning-models-385496cf7cee
"""

# TODO: Importing necessary library
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
# Assuming data is already preprocessed and split
#X = df.drop('target_variable', axis=1)
#Y = df['target_variable']
#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# TODO: Create a Logistic Regression Classifier
log_reg = LogisticRegression(random_state=42)

# TODO: Train the model
log_reg.fit(X_train, Y_train)

# TODO: Make predictions
Y_pred_log_reg = log_reg.predict(X_test)

# TODO: Print the confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(Y_test, Y_pred_log_reg))

# TODO: Print the classification report
print("\nClassification Report:")
print(classification_report(Y_test, Y_pred_log_reg))

# TODO: Print the model accuracy
print("\nModel Accuracy:")
print(log_reg.score(X_test, Y_test))

"""**Step 3:** Train and Test Gradient Boosting Classifier Model

Check out this resource on Gradient Boosting Classifier - https://blog.paperspace.com/gradient-boosting-for-classification/

**Step 4:** Check Classification Report and Confusion Matrix
"""

# Importing necessary library
from sklearn.ensemble import GradientBoostingClassifier

# Assuming data is already preprocessed and split
#X = df.drop('target_variable', axis=1)
#Y = df['target_variable']
#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# TODO:Create a Gradient Boosting Classifier
gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)

# TODO:Train the model
gb_clf.fit(X_train, Y_train)

# TODO:Make predictions
Y_pred_gb = gb_clf.predict(X_test)

# TODO:Print the confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(Y_test, Y_pred_gb))

# TODO:Print the classification report
print("\nClassification Report:")
print(classification_report(Y_test, Y_pred_gb))

# TODO:Print the model accuracy
print("\nModel Accuracy:")
print(gb_clf.score(X_test, Y_test))

"""**Step 5:** Train and Test XGBoost Model

Check out this resource on XGBoost Model-

https://www.nvidia.com/en-us/glossary/xgboost/#:~:text=What%20is%20XGBoost%3F,%2C%20classification%2C%20and%20ranking%20problems.

**Step 6:** Check Classification Report and Confusion Matrix
"""

# Importing necessary library
import xgboost as xgb
from sklearn.metrics import classification_report, confusion_matrix

# Assuming data is already preprocessed and split
#X = df.drop('target_variable', axis=1)
#Y = df['target_variable']
#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# TODO: Create an XGBoost Classifier
xgb_clf = xgb.XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='mlogloss')

# TODO: Train the model
xgb_clf.fit(X_train, Y_train)

# TODO: Make predictions
Y_pred_xgb = xgb_clf.predict(X_test)

# TODO: Print the confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(Y_test, Y_pred_xgb))

# TODO: Print the classification report
print("\nClassification Report:")
print(classification_report(Y_test, Y_pred_xgb))

# TODO: Print the model accuracy
print("\nModel Accuracy:")
print(xgb_clf.score(X_test, Y_test))

"""**Step 7:** Train and Test SVM Classifier

Check out this resource from IBM on SVM -
https://www.ibm.com/topics/support-vector-machine

**Step 8:** Check Prediction Probabilities, Test Prediction and Score

Want to learn how to calibre prediction scores? Check out this medium article - https://medium.com/walmartglobaltech/calibration-of-the-prediction-scores-how-does-it-help-f9ec79807e9
"""

from sklearn.svm import SVC

# TODO: Create and train the model
svm_clf = SVC(probability=True, random_state=42)
svm_clf.fit(X_train, Y_train)

# TODO: Predictions
print("SVM Predictions:", svm_clf.predict([[68, 36, 193, 193, 31]]))
print("SVM Prediction Probabilities:", svm_clf.predict_proba([[68, 36, 193, 193, 31]]))
print("SVM Test Predictions:", svm_clf.predict(X_test))
print("SVM Score:", svm_clf.score(X_test, Y_test))

"""**Step 9:** Train and Test KNN Classifier

Check out this resource from IBM on KNN - https://www.ibm.com/topics/knn

**Step 10:** Check Prediction Probabilities, Test Prediction and Score
"""

from sklearn.neighbors import KNeighborsClassifier

# TODO: Create and train the model
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, Y_train)

# TODO: Predictions
print("KNN Predictions:", knn_clf.predict([[68, 36, 193, 193, 31]]))
print("KNN Prediction Probabilities:", knn_clf.predict_proba([[68, 36, 193, 193, 31]]))
print("KNN Test Predictions:", knn_clf.predict(X_test))
print("KNN Score:", knn_clf.score(X_test, Y_test))

"""**Step 11:** Train and Test Decision Tree Classifier

Check out this resource from IBM on Decision Tree Classifier - https://ibm.com/topics/decision-trees

**Step 12:** Check Prediction Probabilities, Test Prediction and Score
"""

from sklearn.tree import DecisionTreeClassifier

# TODO: Create and train the model
dt_clf = DecisionTreeClassifier(random_state=42)
dt_clf.fit(X_train, Y_train)

# TODO: Predictions
print("Decision Tree Test Prediction:", dt_clf.predict([[68, 36, 193, 193, 31]]))
print("Decision Tree Probability Prediction:", dt_clf.predict_proba([[68, 36, 193, 193, 31]]))
print("Decision Tree Test Predictions:", dt_clf.predict(X_test))
print("Decision Tree Score:", dt_clf.score(X_test, Y_test))

"""**Step 13:** Train and Test LightGBM Model

Check out this resource on LightGBM Model from geeksforgeeks - https://www.geeksforgeeks.org/lightgbm-light-gradient-boosting-machine/

**Step 14:** Check Prediction Probabilities, Test Prediction and Score
"""

import lightgbm as lgb

# TODO: Create and train the model
lgb_clf = lgb.LGBMClassifier(n_estimators=100, random_state=42)
lgb_clf.fit(X_train, Y_train)

# TODO: Predictions
print("LightGBM Predictions:", lgb_clf.predict([[68, 36, 193, 193, 31]]))
print("LightGBM Prediction Probabilities:", lgb_clf.predict_proba([[68, 36, 193, 193, 31]]))
print("LightGBM Test Predictions:", lgb_clf.predict(X_test))
print("LightGBM Score:", lgb_clf.score(X_test, Y_test))

"""**Step 15**: Train and Test AdaBoostClassifier Model

Want to learn more about AdaBoost Classifier? Check out this article from datacamp - https://www.datacamp.com/tutorial/adaboost-classifier-python

**Step 16**: Check Prediction Probabilities, Test Prediction and Score
"""

from sklearn.ensemble import AdaBoostClassifier

# TODO: Create and train the model
ada_clf = AdaBoostClassifier(n_estimators=100, random_state=42)
ada_clf.fit(X_train, Y_train)

# TODO: Predictions
print("AdaBoost Predictions:", ada_clf.predict([[68, 36, 193, 193, 31]]))
print("AdaBoost Prediction Probabilities:", ada_clf.predict_proba([[68, 36, 193, 193, 31]]))
print("AdaBoost Test Predictions:", ada_clf.predict(X_test))
print("AdaBoost Score:", ada_clf.score(X_test, Y_test))

"""## Milestone 7: Model Evaluation

**Goal:** Evaluate the performance of the trained Random Forest model using metrics such as accuracy, precision, recall, and F1-score. Apply ROC - RECEIVER OPERATING CHARACTERISTIC CURVE. This step ensures that the model is effective in making predictions.

Want to learn more about ROC? Check out this youtube video- https://www.youtube.com/watch?v=z5qA9qZMyw0
"""

!pip install catboost
!pip install xgboost
!pip install lightgbm

# TODO:Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# TODO: Load your dataset
data = pd.read_csv("/content/data.csv")


# TODO: Print the columns to verify
print("Columns in DataFrame:", data.columns)

# TODO: Replace 'symptoms1', 'symptoms2', etc., with actual column names if different
symptom_columns = ['symptoms1', 'symptoms2', 'symptoms3', 'symptoms4', 'symptoms5']
target_column = 'Dangerous'

# Check if the columns exist in the DataFrame
for col in symptom_columns + [target_column]:
    if col not in data.columns:
        raise KeyError(f"Column '{col}' not found in the dataset")

# TODO: Encoding categorical variables
LE = LabelEncoder()
for col in symptom_columns:
    data[f'enc-{col}'] = LE.fit_transform(data[col])
data[f'enc-{target_column}'] = LE.fit_transform(data[target_column])

# TODO: Preparing data
X = data[[f'enc-{col}' for col in symptom_columns]]
Y = data[f'enc-{target_column}']

# TODO: Splitting data into train and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# TODO: Function to evaluate model
def evaluate_model(model, X_test, Y_test, model_name):
    Y_pred = model.predict(X_test)
    Y_pred_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    accuracy = accuracy_score(Y_test, Y_pred)
    precision = precision_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred)
    f1 = f1_score(Y_test, Y_pred)
    roc_auc = roc_auc_score(Y_test, Y_pred_prob) if Y_pred_prob is not None else None

    print(f"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, ROC AUC: {roc_auc:.4f}")

    return Y_pred_prob

# TODO: Function to plot ROC curve
def plot_roc_curve(models, X_test, Y_test):
    plt.figure(figsize=(10, 8))
    for model, name in models:
        Y_pred_prob = evaluate_model(model, X_test, Y_test, name)
        if Y_pred_prob is not None:
            fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)
            plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc_score(Y_test, Y_pred_prob):.4f})')

    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()

# TODO: Define models
models = [
    (RandomForestClassifier(random_state=42), "Random Forest"),
    (xgb.XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='mlogloss'), "XGBoost"),
    (lgb.LGBMClassifier(n_estimators=100, random_state=42), "LightGBM"),
    (CatBoostClassifier(n_estimators=100, random_state=42, verbose=0), "CatBoost"),
    (AdaBoostClassifier(n_estimators=100, random_state=42), "AdaBoost"),
    (GradientBoostingClassifier(n_estimators=100, random_state=42), "Gradient Boosting")
]

# TODO: Train and evaluate each model
for model, name in models:
    model.fit(X_train, Y_train)
    evaluate_model(model, X_test, Y_test, name)

# Plot ROC Curve for each model
plot_roc_curve(models, X_test, Y_test)